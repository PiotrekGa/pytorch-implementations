{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piotrgabrys/.local/share/virtualenvs/magic-Ij777sAq/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    vocab_size = tokenizer.vocab_size + 1\n",
    "    ignored_index = tokenizer.vocab_size\n",
    "    block_size = 20\n",
    "    emb_dim = 16\n",
    "    d_model = 16\n",
    "    m_model = 8\n",
    "    n_heads = 4\n",
    "    lr = 0.01\n",
    "    n_epochs = 5\n",
    "    batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, split, block_size, vocab_size, transform=None, target_transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        dataset = load_dataset(\"glue\", \"mrpc\", split=split)\n",
    "        dataset = [tokenizer(i['sentence1'])['input_ids'] for i in dataset]\n",
    "        self.dataset = [self.pad(x) for x in dataset]\n",
    "        del dataset\n",
    "    \n",
    "    def pad(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        if len(x) == self.block_size + 1:\n",
    "            return x\n",
    "        elif len(x) > self.block_size + 1:\n",
    "            idx = torch.randint(len(x) - self.block_size - 1, (1,))\n",
    "            return x[idx: idx + self.block_size + 1]\n",
    "        else:\n",
    "            n_to_pad = self.block_size + 1 - len(x)\n",
    "            x = F.pad(x, (0, n_to_pad), 'constant', self.vocab_size - 1)\n",
    "            return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx][:-1], self.dataset[idx][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/piotrgabrys/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/Users/piotrgabrys/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "train_ds = TextDataset('train', CONFIG.block_size, CONFIG.vocab_size)\n",
    "valid_ds = TextDataset('validation', CONFIG.block_size, CONFIG.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, CONFIG.batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_ds, CONFIG.batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSM(nn.Module):\n",
    "    def __init__(self, d_model, m_model, mode) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.m_model = m_model\n",
    "        self.mode = mode\n",
    "        \n",
    "        if self.mode == 'diag':\n",
    "            A = torch.randn((self.d_model, self.m_model)) / 100\n",
    "        elif self.mode == 'shift':\n",
    "            A = torch.cat([torch.randn((self.d_model, self.m_model - 1)), torch.zeros((self.d_model, 1))], dim=1)\n",
    "        else:\n",
    "            raise ValueError('not such mode')\n",
    "        self.A = nn.Parameter(A)\n",
    "        B = torch.randn((self.d_model, self.m_model)) / 100\n",
    "        self.B = nn.Parameter(B)\n",
    "        C = torch.randn((self.d_model, self.m_model)) / 100\n",
    "        self.C = nn.Parameter(C)\n",
    "        D = torch.randn((self.d_model)) / 100\n",
    "        self.D = nn.Parameter(D)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B T C\n",
    "        block_size = x.shape[1]\n",
    "        filter = [(self.C * (self.A ** i) * self.B).sum(1) for i in range(block_size)]\n",
    "        filter = torch.stack(filter, dim=1).squeeze()\n",
    "        filter = rearrange(filter, 'b c -> c b')\n",
    "        x = x.flip(1)\n",
    "        conv_part = filter * x\n",
    "        conv = conv_part.cumsum(dim=1)\n",
    "        conv = conv.flip(1)\n",
    "\n",
    "        y = conv + self.D * x\n",
    "\n",
    "        # batch, block, emb_dim\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3Model(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, d_model, m_model, n_heads) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.d_model = d_model\n",
    "        self.m_model = m_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "\n",
    "        self.k_proj = nn.Linear(self.emb_dim, self.d_model, bias=False)\n",
    "        nn.init.xavier_normal_(self.k_proj.weight)\n",
    "        self.ln1 = nn.LayerNorm(self.d_model)\n",
    "        self.ssm_shift = SSM(self.d_model, self.m_model, 'shift')\n",
    "\n",
    "        self.v_proj = nn.Linear(self.emb_dim, self.d_model, bias=False)\n",
    "        nn.init.xavier_normal_(self.v_proj.weight)\n",
    "        self.ln2 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.vhs_proj = nn.ModuleList([nn.Linear(self.d_model, int(self.d_model / n_heads), bias=False) for _ in range(n_heads)])\n",
    "        for vh_proj in self.vhs_proj:\n",
    "            nn.init.xavier_normal_(vh_proj.weight)\n",
    "\n",
    "        self.ssm_diags = nn.ModuleList([SSM(int(self.d_model / n_heads), self.m_model, 'diag') for _ in range(self.n_heads)])\n",
    "\n",
    "        self.q_proj = nn.Linear(self.emb_dim, self.d_model, bias=False)\n",
    "        nn.init.xavier_normal_(self.q_proj.weight)\n",
    "        self.ln3 = nn.LayerNorm(self.d_model)\n",
    "\n",
    "        self.output_layer = nn.Linear(self.d_model, self.vocab_size)\n",
    "        nn.init.xavier_normal_(self.output_layer.weight)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, T\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        k = self.ln1(self.k_proj(x))\n",
    "        k = self.ssm_shift(k)\n",
    "\n",
    "        v = self.ln2(self.v_proj(x))\n",
    "\n",
    "        v = v * k\n",
    "        v = [ssm_diag(self.vhs_proj[i](v)) for i, ssm_diag in enumerate(self.ssm_diags)]\n",
    "        v = torch.cat(v, dim=2)\n",
    "\n",
    "        q = self.ln3(self.q_proj(x))\n",
    "        out = q * v\n",
    "\n",
    "        out = self.output_layer(out)\n",
    "\n",
    "        out = rearrange(out, 'B T C -> (B T) C')\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = H3Model(vocab_size=CONFIG.vocab_size, emb_dim=CONFIG.emb_dim, d_model=CONFIG.d_model, m_model=CONFIG.m_model, n_heads=CONFIG.n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(ignore_index=CONFIG.ignored_index)\n",
    "optimizer = torch.optim.Adam(model.parameters(), CONFIG.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        preds = model(x)\n",
    "        train_loss += loss_func(preds, y.ravel())\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    valid_loss = 0\n",
    "    for x, y in valid_loader:\n",
    "        preds = model(x)\n",
    "        valid_loss += loss_func(preds, y.ravel())\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0, train loss: 6.869299, valid loss: 7.389570\n",
      "EPOCH 1, train loss: 6.076054, valid loss: 7.140264\n",
      "EPOCH 2, train loss: 5.549601, valid loss: 7.070755\n",
      "EPOCH 3, train loss: 5.123650, valid loss: 7.155102\n",
      "EPOCH 4, train loss: 4.780811, valid loss: 7.360164\n"
     ]
    }
   ],
   "source": [
    "for i in range(CONFIG.n_epochs):\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y.ravel())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss, valid_loss = estimate_loss()\n",
    "    print(f'EPOCH {i}, train loss: {train_loss:.6f}, valid loss: {valid_loss:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magic-Ij777sAq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e455ffde316e0e59f235fb716f046746ab75667a470758045ed86161d928d35d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
