{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piotrgabrys/.local/share/virtualenvs/magic-Ij777sAq/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from einops import rearrange, repeat\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import numpy as np\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    vocab_size = tokenizer.vocab_size + 1\n",
    "    ignored_index = tokenizer.vocab_size\n",
    "    block_size = 20\n",
    "    emb_dim = 16\n",
    "    d_model = 16\n",
    "    m_model = 16\n",
    "    n_heads = 4\n",
    "    n_ssm_heads = 2\n",
    "    n_layer = 2\n",
    "    lr = 0.01\n",
    "    n_epochs = 5\n",
    "    batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, split, block_size, vocab_size, transform=None, target_transform=None) -> None:\n",
    "        super().__init__()\n",
    "        self.split = split\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        dataset = load_dataset(\"glue\", \"mrpc\", split=split)\n",
    "        dataset = [tokenizer(i['sentence1'])['input_ids'] for i in dataset]\n",
    "        self.dataset = [self.pad(x) for x in dataset]\n",
    "        del dataset\n",
    "    \n",
    "    def pad(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        if len(x) == self.block_size + 1:\n",
    "            return x\n",
    "        elif len(x) > self.block_size + 1:\n",
    "            idx = torch.randint(len(x) - self.block_size - 1, (1,))\n",
    "            return x[idx: idx + self.block_size + 1]\n",
    "        else:\n",
    "            n_to_pad = self.block_size + 1 - len(x)\n",
    "            x = F.pad(x, (0, n_to_pad), 'constant', self.vocab_size - 1)\n",
    "            return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[idx][:-1], self.dataset[idx][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/piotrgabrys/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "Found cached dataset glue (/Users/piotrgabrys/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    }
   ],
   "source": [
    "train_ds = TextDataset('train', CONFIG.block_size, CONFIG.vocab_size)\n",
    "valid_ds = TextDataset('validation', CONFIG.block_size, CONFIG.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, CONFIG.batch_size, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(valid_ds, CONFIG.batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSM(nn.Module):\n",
    "    def __init__(self, d_model, m_model, mode) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.m_model = m_model\n",
    "        self.mode = mode\n",
    "        \n",
    "        if self.mode == 'diag':\n",
    "            A = torch.randn((self.d_model, self.m_model)) / 100\n",
    "        elif self.mode == 'shift':\n",
    "            A = torch.cat([torch.randn((self.d_model, self.m_model - 1)), torch.zeros((self.d_model, 1))], dim=1)\n",
    "        else:\n",
    "            raise ValueError('not such mode')\n",
    "        self.A = nn.Parameter(A)\n",
    "        B = torch.randn((self.d_model, self.m_model)) / 100\n",
    "        self.B = nn.Parameter(B)\n",
    "        C = torch.randn((self.d_model, self.m_model)) / 100\n",
    "        self.C = nn.Parameter(C)\n",
    "        D = torch.randn((self.d_model)) / 100\n",
    "        self.D = nn.Parameter(D)\n",
    "        E = torch.eye(self.m_model)\n",
    "        if self.mode == 'shift':\n",
    "            E = torch.cat([E[:,1:], torch.zeros(m_model, 1)], dim=1)\n",
    "        E = repeat(E, 'h w -> n h w', n=self.d_model)\n",
    "        self.register_buffer('E', E)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B T C\n",
    "        block_size = x.shape[1]\n",
    "        filter = [(self.C * (self.A ** i) * self.B).sum(1) for i in range(block_size)]\n",
    "        filter = torch.stack(filter, dim=1).squeeze()\n",
    "        filter = rearrange(filter, 'b c -> c b')\n",
    "        x = x.flip(1)\n",
    "        conv_part = filter * x\n",
    "        conv = conv_part.cumsum(dim=1)\n",
    "        conv = conv.flip(1)\n",
    "\n",
    "        y = conv + self.D * x\n",
    "\n",
    "        # batch, block, emb_dim\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3Head(nn.Module):\n",
    "    \"\"\" one head of H3 \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, head_size, n_ssm_heads, m_model):\n",
    "        super().__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.head_size = head_size\n",
    "        self.n_ssm_heads = n_ssm_heads\n",
    "        self.m_model = m_model\n",
    "\n",
    "        self.key = nn.Linear(self.emb_size, head_size, bias=False)\n",
    "        self.ln_key = nn.LayerNorm(head_size)\n",
    "        self.query = nn.Linear(self.emb_size, head_size, bias=False)\n",
    "        self.ln_query = nn.LayerNorm(head_size)\n",
    "        self.value = nn.Linear(self.emb_size, head_size, bias=False)\n",
    "        self.ln_value = nn.LayerNorm(head_size)\n",
    "        self.ssm_shift = SSM(self.head_size, self.m_model, 'shift')\n",
    "\n",
    "        self.values = nn.ModuleList([nn.Linear(self.head_size, int(self.head_size / self.n_ssm_heads), bias=False) for _ in range(self.n_ssm_heads)])\n",
    "        self.ssm_diags = nn.ModuleList([SSM(int(self.head_size / self.n_ssm_heads), self.m_model, 'diag') for _ in range(self.n_ssm_heads)])\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        k = self.key(x)\n",
    "        k = self.ln_key(k)\n",
    "        k = self.ssm_shift(k)\n",
    "        v = self.value(x)\n",
    "        v = self.ln_key(v)\n",
    "        v = v * k\n",
    "        v = torch.cat([ssm_diag(value(v)) for value, ssm_diag in zip(self.values, self.ssm_diags)], dim=2)\n",
    "        q = self.query(x)\n",
    "        q = self.ln_key(q)\n",
    "        q = v * q\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadH3(nn.Module):\n",
    "    \"\"\" multiple heads of H3 in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, num_heads, head_size, n_ssm_heads, m_model, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([H3Head(emb_size, head_size, n_ssm_heads, m_model) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, emb_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3Block(nn.Module):\n",
    "    \"\"\" H3 block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, emb_size, num_heads, n_ssm_heads, m_model):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = emb_size // num_heads\n",
    "        self.sa = MultiHeadH3(emb_size, num_heads, head_size, n_ssm_heads, m_model)\n",
    "        self.ffwd = FeedFoward(emb_size)\n",
    "        self.ln1 = nn.LayerNorm(emb_size)\n",
    "        self.ln2 = nn.LayerNorm(emb_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H3LanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, d_model, m_model, n_heads, n_ssm_heads, n_layer) -> None:\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.d_model = d_model\n",
    "        self.m_model = m_model\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.emb = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "\n",
    "        self.blocks = nn.Sequential(*[H3Block(emb_dim, n_heads, n_ssm_heads, m_model) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(emb_dim) # final layer norm\n",
    "        self.lm_head = nn.Linear(emb_dim, vocab_size)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # B, T\n",
    "        x = self.emb(x)\n",
    "        \n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        x = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        x = rearrange(x, 'B T vocab -> (B T) vocab')\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = H3LanguageModel(vocab_size=CONFIG.vocab_size, emb_dim=CONFIG.emb_dim, d_model=CONFIG.d_model, m_model=CONFIG.m_model, n_heads=CONFIG.n_heads, n_ssm_heads=CONFIG.n_ssm_heads, n_layer=CONFIG.n_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(ignore_index=CONFIG.ignored_index)\n",
    "optimizer = torch.optim.Adam(model.parameters(), CONFIG.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "\n",
    "    train_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        preds = model(x)\n",
    "        train_loss += loss_func(preds, y.ravel())\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    valid_loss = 0\n",
    "    for x, y in valid_loader:\n",
    "        preds = model(x)\n",
    "        valid_loss += loss_func(preds, y.ravel())\n",
    "    valid_loss /= len(valid_loader)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return train_loss, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0, train loss: 6.720024, valid loss: 7.207736\n",
      "EPOCH 1, train loss: 6.026439, valid loss: 6.909051\n",
      "EPOCH 2, train loss: 5.356634, valid loss: 6.824624\n",
      "EPOCH 3, train loss: 4.846189, valid loss: 6.923506\n",
      "EPOCH 4, train loss: 4.437585, valid loss: 7.060393\n"
     ]
    }
   ],
   "source": [
    "for i in range(CONFIG.n_epochs):\n",
    "    for x, y in train_loader:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        preds = model(x)\n",
    "        loss = loss_func(preds, y.ravel())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss, valid_loss = estimate_loss()\n",
    "    print(f'EPOCH {i}, train loss: {train_loss:.6f}, valid loss: {valid_loss:.6f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magic-Ij777sAq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e455ffde316e0e59f235fb716f046746ab75667a470758045ed86161d928d35d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
