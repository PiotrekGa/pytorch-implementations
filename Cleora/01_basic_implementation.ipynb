{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleora: A Simple, Strong and Scalable Graph Embedding Scheme\n",
    "\n",
    "https://arxiv.org/abs/2102.02302\n",
    "\n",
    "https://github.com/Synerise/cleora"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to implement Cleora with torch. I've tested it on text data. The embedding computes in 15.7 sec on my Laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piotrgabrys/.local/share/virtualenvs/pytorch-implementations-X85rwZ0n/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (/Users/piotrgabrys/.cache/huggingface/datasets/wikitext/wikitext-103-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n",
      "100%|██████████| 3/3 [00:00<00:00,  4.34it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('wikitext', 'wikitext-103-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 647871/1801350 [04:37<08:23, 2290.55it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (1059 > 1024). Running this sequence through the model will result in indexing errors\n",
      "100%|██████████| 1801350/1801350 [12:25<00:00, 2415.97it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = [tokenizer(i)['input_ids'] for i in tqdm(dataset['train']['text']) if len(i) > 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1081822/1081822 [00:04<00:00, 252803.61it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset2 = []\n",
    "for sentence in tqdm(dataset):\n",
    "    if len(sentence) > 10:\n",
    "        start = random.randint(0, len(sentence) - 10)\n",
    "        end = start + 10\n",
    "        dataset2.append(sentence[start:end])\n",
    "    else:\n",
    "        dataset2.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Series(dataset2).explode().to_frame().reset_index()\n",
    "df.columns = ['sentence_id', 'token']\n",
    "df.to_csv('sentences.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sentences.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(df.token)\n",
    "df.token = le.transform(df.token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, df, left_on='sentence_id', right_on='sentence_id')\n",
    "df2 = df.groupby(['token_x', 'token_y']).count()\n",
    "df2.reset_index(inplace=True)\n",
    "df2.columns = ['token_x', 'token_y', 'cnt']\n",
    "total_cnt = df2.groupby('token_x').sum()['cnt']\n",
    "df2['total_cnt'] = df2.token_x.map(total_cnt)\n",
    "df2['prob'] = df2.cnt / df2.total_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_x</th>\n",
       "      <th>token_y</th>\n",
       "      <th>cnt</th>\n",
       "      <th>total_cnt</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>40</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>0.050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token_x  token_y  cnt  total_cnt   prob\n",
       "0        0        0    4         40  0.100\n",
       "1        0        2    1         40  0.025\n",
       "2        0       15    1         40  0.025\n",
       "3        0       51    1         40  0.025\n",
       "4        0       55    2         40  0.050"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize embeddings and create sparse adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4p/qc9gvsv157v006qgb_zchlzh0000gn/T/ipykernel_60040/1679344108.py:7: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/SparseCsrTensorImpl.cpp:56.)\n",
      "  transition_matrix = transition_matrix.to_sparse_csr()\n"
     ]
    }
   ],
   "source": [
    "emb_size = 100\n",
    "\n",
    "indices = torch.tensor(df2.loc[:, ['token_x', 'token_y']].values).T\n",
    "values = torch.tensor(df2.loc[:, 'prob'].values)\n",
    "size = indices.max().item() + 1\n",
    "transition_matrix = torch.sparse_coo_tensor(indices=indices, values=values, size=(size, size), dtype=torch.float32, requires_grad=False)\n",
    "transition_matrix = transition_matrix.to_sparse_csr()\n",
    "\n",
    "torch.random.manual_seed(42)\n",
    "embedding = torch.randn((size, emb_size), dtype=torch.float32, requires_grad=False)\n",
    "embedding = F.normalize(embedding, p=2, dim=1)\n",
    "embedding = embedding.to_sparse_csr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, df2, total_cnt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(i)\n",
    "    embedding = transition_matrix @ embedding\n",
    "    embedding = F.normalize(embedding.to_dense(), p=2, dim=1).to_sparse_csr()\n",
    "embedding = embedding.to_dense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = pd.DataFrame(embedding.numpy(), index=le.classes_)\n",
    "words = [tokenizer.decode(i) for i in le.classes_]\n",
    "embedding.index = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>'</th>\n",
       "      <td>-0.078871</td>\n",
       "      <td>0.101386</td>\n",
       "      <td>-0.122783</td>\n",
       "      <td>-0.128704</td>\n",
       "      <td>0.013432</td>\n",
       "      <td>-0.090238</td>\n",
       "      <td>0.068177</td>\n",
       "      <td>0.072422</td>\n",
       "      <td>-0.012252</td>\n",
       "      <td>0.019230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006720</td>\n",
       "      <td>0.042587</td>\n",
       "      <td>0.086443</td>\n",
       "      <td>0.148699</td>\n",
       "      <td>0.060983</td>\n",
       "      <td>-0.105272</td>\n",
       "      <td>0.056365</td>\n",
       "      <td>0.101028</td>\n",
       "      <td>-0.084708</td>\n",
       "      <td>0.096045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.077256</td>\n",
       "      <td>0.111697</td>\n",
       "      <td>-0.114209</td>\n",
       "      <td>-0.130592</td>\n",
       "      <td>0.021396</td>\n",
       "      <td>-0.083489</td>\n",
       "      <td>0.059185</td>\n",
       "      <td>0.075306</td>\n",
       "      <td>-0.013261</td>\n",
       "      <td>0.027371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004992</td>\n",
       "      <td>0.045245</td>\n",
       "      <td>0.088574</td>\n",
       "      <td>0.153976</td>\n",
       "      <td>0.060455</td>\n",
       "      <td>-0.100197</td>\n",
       "      <td>0.057497</td>\n",
       "      <td>0.105100</td>\n",
       "      <td>-0.077266</td>\n",
       "      <td>0.095184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-</th>\n",
       "      <td>-0.078081</td>\n",
       "      <td>0.106146</td>\n",
       "      <td>-0.120282</td>\n",
       "      <td>-0.128122</td>\n",
       "      <td>0.017603</td>\n",
       "      <td>-0.087731</td>\n",
       "      <td>0.064212</td>\n",
       "      <td>0.076965</td>\n",
       "      <td>-0.015866</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006208</td>\n",
       "      <td>0.047714</td>\n",
       "      <td>0.088965</td>\n",
       "      <td>0.149447</td>\n",
       "      <td>0.057769</td>\n",
       "      <td>-0.102568</td>\n",
       "      <td>0.059099</td>\n",
       "      <td>0.106120</td>\n",
       "      <td>-0.082972</td>\n",
       "      <td>0.095846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.079771</td>\n",
       "      <td>0.102860</td>\n",
       "      <td>-0.121698</td>\n",
       "      <td>-0.126472</td>\n",
       "      <td>0.017008</td>\n",
       "      <td>-0.088676</td>\n",
       "      <td>0.064447</td>\n",
       "      <td>0.075584</td>\n",
       "      <td>-0.016992</td>\n",
       "      <td>0.018370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006672</td>\n",
       "      <td>0.049668</td>\n",
       "      <td>0.090899</td>\n",
       "      <td>0.147907</td>\n",
       "      <td>0.057339</td>\n",
       "      <td>-0.103476</td>\n",
       "      <td>0.059010</td>\n",
       "      <td>0.105894</td>\n",
       "      <td>-0.085030</td>\n",
       "      <td>0.097719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.079406</td>\n",
       "      <td>0.103333</td>\n",
       "      <td>-0.120526</td>\n",
       "      <td>-0.127985</td>\n",
       "      <td>0.017730</td>\n",
       "      <td>-0.087394</td>\n",
       "      <td>0.063860</td>\n",
       "      <td>0.074601</td>\n",
       "      <td>-0.014871</td>\n",
       "      <td>0.020180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.047096</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.149544</td>\n",
       "      <td>0.059204</td>\n",
       "      <td>-0.103544</td>\n",
       "      <td>0.058020</td>\n",
       "      <td>0.103905</td>\n",
       "      <td>-0.082364</td>\n",
       "      <td>0.095828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amplification</th>\n",
       "      <td>-0.079749</td>\n",
       "      <td>0.097286</td>\n",
       "      <td>-0.127882</td>\n",
       "      <td>-0.123563</td>\n",
       "      <td>0.013753</td>\n",
       "      <td>-0.092153</td>\n",
       "      <td>0.068693</td>\n",
       "      <td>0.079774</td>\n",
       "      <td>-0.020657</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008155</td>\n",
       "      <td>0.052868</td>\n",
       "      <td>0.092107</td>\n",
       "      <td>0.143906</td>\n",
       "      <td>0.054667</td>\n",
       "      <td>-0.105253</td>\n",
       "      <td>0.060384</td>\n",
       "      <td>0.109164</td>\n",
       "      <td>-0.090111</td>\n",
       "      <td>0.097654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ominated</th>\n",
       "      <td>-0.078943</td>\n",
       "      <td>0.101755</td>\n",
       "      <td>-0.120527</td>\n",
       "      <td>-0.132504</td>\n",
       "      <td>0.013220</td>\n",
       "      <td>-0.090621</td>\n",
       "      <td>0.068843</td>\n",
       "      <td>0.071721</td>\n",
       "      <td>-0.007848</td>\n",
       "      <td>0.024456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004701</td>\n",
       "      <td>0.036491</td>\n",
       "      <td>0.082882</td>\n",
       "      <td>0.150605</td>\n",
       "      <td>0.065203</td>\n",
       "      <td>-0.105306</td>\n",
       "      <td>0.054782</td>\n",
       "      <td>0.098239</td>\n",
       "      <td>-0.081994</td>\n",
       "      <td>0.093821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>regress</th>\n",
       "      <td>-0.078452</td>\n",
       "      <td>0.103818</td>\n",
       "      <td>-0.118927</td>\n",
       "      <td>-0.134839</td>\n",
       "      <td>0.012833</td>\n",
       "      <td>-0.090133</td>\n",
       "      <td>0.070644</td>\n",
       "      <td>0.069622</td>\n",
       "      <td>-0.003385</td>\n",
       "      <td>0.029162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003307</td>\n",
       "      <td>0.030045</td>\n",
       "      <td>0.078554</td>\n",
       "      <td>0.152287</td>\n",
       "      <td>0.068324</td>\n",
       "      <td>-0.104958</td>\n",
       "      <td>0.053035</td>\n",
       "      <td>0.095629</td>\n",
       "      <td>-0.079955</td>\n",
       "      <td>0.092081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Collider</th>\n",
       "      <td>-0.082631</td>\n",
       "      <td>0.086775</td>\n",
       "      <td>-0.138616</td>\n",
       "      <td>-0.106659</td>\n",
       "      <td>0.015481</td>\n",
       "      <td>-0.092726</td>\n",
       "      <td>0.064990</td>\n",
       "      <td>0.090261</td>\n",
       "      <td>-0.043482</td>\n",
       "      <td>-0.016299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014213</td>\n",
       "      <td>0.083852</td>\n",
       "      <td>0.110063</td>\n",
       "      <td>0.131640</td>\n",
       "      <td>0.033698</td>\n",
       "      <td>-0.105092</td>\n",
       "      <td>0.070027</td>\n",
       "      <td>0.124329</td>\n",
       "      <td>-0.103159</td>\n",
       "      <td>0.104426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>informants</th>\n",
       "      <td>-0.079078</td>\n",
       "      <td>0.103156</td>\n",
       "      <td>-0.120297</td>\n",
       "      <td>-0.131849</td>\n",
       "      <td>0.013620</td>\n",
       "      <td>-0.090972</td>\n",
       "      <td>0.069131</td>\n",
       "      <td>0.071633</td>\n",
       "      <td>-0.007755</td>\n",
       "      <td>0.024775</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004798</td>\n",
       "      <td>0.036472</td>\n",
       "      <td>0.082583</td>\n",
       "      <td>0.150350</td>\n",
       "      <td>0.065386</td>\n",
       "      <td>-0.104664</td>\n",
       "      <td>0.054770</td>\n",
       "      <td>0.098971</td>\n",
       "      <td>-0.082108</td>\n",
       "      <td>0.094304</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>42451 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0         1         2         3         4         5   \\\n",
       "'              -0.078871  0.101386 -0.122783 -0.128704  0.013432 -0.090238   \n",
       ",              -0.077256  0.111697 -0.114209 -0.130592  0.021396 -0.083489   \n",
       "-              -0.078081  0.106146 -0.120282 -0.128122  0.017603 -0.087731   \n",
       ".              -0.079771  0.102860 -0.121698 -0.126472  0.017008 -0.088676   \n",
       "0              -0.079406  0.103333 -0.120526 -0.127985  0.017730 -0.087394   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       " amplification -0.079749  0.097286 -0.127882 -0.123563  0.013753 -0.092153   \n",
       "ominated       -0.078943  0.101755 -0.120527 -0.132504  0.013220 -0.090621   \n",
       " regress       -0.078452  0.103818 -0.118927 -0.134839  0.012833 -0.090133   \n",
       " Collider      -0.082631  0.086775 -0.138616 -0.106659  0.015481 -0.092726   \n",
       " informants    -0.079078  0.103156 -0.120297 -0.131849  0.013620 -0.090972   \n",
       "\n",
       "                      6         7         8         9   ...        90  \\\n",
       "'               0.068177  0.072422 -0.012252  0.019230  ...  0.006720   \n",
       ",               0.059185  0.075306 -0.013261  0.027371  ...  0.004992   \n",
       "-               0.064212  0.076965 -0.015866  0.019608  ...  0.006208   \n",
       ".               0.064447  0.075584 -0.016992  0.018370  ...  0.006672   \n",
       "0               0.063860  0.074601 -0.014871  0.020180  ...  0.006794   \n",
       "...                  ...       ...       ...       ...  ...       ...   \n",
       " amplification  0.068693  0.079774 -0.020657  0.009400  ...  0.008155   \n",
       "ominated        0.068843  0.071721 -0.007848  0.024456  ...  0.004701   \n",
       " regress        0.070644  0.069622 -0.003385  0.029162  ...  0.003307   \n",
       " Collider       0.064990  0.090261 -0.043482 -0.016299  ...  0.014213   \n",
       " informants     0.069131  0.071633 -0.007755  0.024775  ...  0.004798   \n",
       "\n",
       "                      91        92        93        94        95        96  \\\n",
       "'               0.042587  0.086443  0.148699  0.060983 -0.105272  0.056365   \n",
       ",               0.045245  0.088574  0.153976  0.060455 -0.100197  0.057497   \n",
       "-               0.047714  0.088965  0.149447  0.057769 -0.102568  0.059099   \n",
       ".               0.049668  0.090899  0.147907  0.057339 -0.103476  0.059010   \n",
       "0               0.047096  0.090067  0.149544  0.059204 -0.103544  0.058020   \n",
       "...                  ...       ...       ...       ...       ...       ...   \n",
       " amplification  0.052868  0.092107  0.143906  0.054667 -0.105253  0.060384   \n",
       "ominated        0.036491  0.082882  0.150605  0.065203 -0.105306  0.054782   \n",
       " regress        0.030045  0.078554  0.152287  0.068324 -0.104958  0.053035   \n",
       " Collider       0.083852  0.110063  0.131640  0.033698 -0.105092  0.070027   \n",
       " informants     0.036472  0.082583  0.150350  0.065386 -0.104664  0.054770   \n",
       "\n",
       "                      97        98        99  \n",
       "'               0.101028 -0.084708  0.096045  \n",
       ",               0.105100 -0.077266  0.095184  \n",
       "-               0.106120 -0.082972  0.095846  \n",
       ".               0.105894 -0.085030  0.097719  \n",
       "0               0.103905 -0.082364  0.095828  \n",
       "...                  ...       ...       ...  \n",
       " amplification  0.109164 -0.090111  0.097654  \n",
       "ominated        0.098239 -0.081994  0.093821  \n",
       " regress        0.095629 -0.079955  0.092081  \n",
       " Collider       0.124329 -0.103159  0.104426  \n",
       " informants     0.098971 -0.082108  0.094304  \n",
       "\n",
       "[42451 rows x 100 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = cosine_distances(embedding.loc[['king'], :].values - embedding.loc[['man'], :].values + embedding.loc[['woman'], :].values, embedding.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " code          0.000002\n",
       " plains        0.000002\n",
       "inating        0.000003\n",
       " dem           0.000003\n",
       " tablets       0.000003\n",
       " kits          0.000004\n",
       " software      0.000004\n",
       " networking    0.000004\n",
       " affiliated    0.000004\n",
       " tracking      0.000004\n",
       "inth           0.000004\n",
       " radio         0.000004\n",
       " Haitian       0.000004\n",
       " lact          0.000004\n",
       " lasers        0.000004\n",
       "human          0.000004\n",
       "inate          0.000004\n",
       " loop          0.000004\n",
       " bases         0.000005\n",
       " photo         0.000005\n",
       " theatrical    0.000005\n",
       "verts          0.000005\n",
       "path           0.000005\n",
       " telescopes    0.000005\n",
       "uffs           0.000005\n",
       " text          0.000005\n",
       "plane          0.000005\n",
       " multip        0.000005\n",
       " educ          0.000005\n",
       " corridor      0.000005\n",
       " secondary     0.000005\n",
       " lenses        0.000006\n",
       " targeting     0.000006\n",
       " Irish         0.000006\n",
       "group          0.000006\n",
       "analy          0.000006\n",
       " aph           0.000006\n",
       "orders         0.000006\n",
       "hers           0.000006\n",
       "horse          0.000006\n",
       " languages     0.000006\n",
       "etric          0.000006\n",
       " benevolent    0.000006\n",
       " stations      0.000006\n",
       "rest           0.000006\n",
       "amina          0.000006\n",
       " brig          0.000006\n",
       " alignment     0.000006\n",
       " Older         0.000007\n",
       " cracking      0.000007\n",
       "dtype: float32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(dists.ravel(), index=embedding.index).sort_values()[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-implementations-X85rwZ0n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d8e212676cc23fe707b6bf2d56de07f4ec60fe848761de43c7b264c969a3736"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
